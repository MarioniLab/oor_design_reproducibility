{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incredible-aaron",
   "metadata": {},
   "source": [
    "# Prepare PBMC dataset from cellxgene portal\n",
    "\n",
    "In this notebook we download and prepare PBMC datasets from published PBMC studies, available from the [cellxgene portal](https://cellxgene.cziscience.com/collections).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "desirable-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=8ed3fa3a3818113f5ae077e206e4b8a29ae2163372659b6a8f4d05f0e50b3f64\n",
      "  Stored in directory: /nfs/users/nfs_e/ed6/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "above-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wget\n",
    "import anndata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conscious-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "## r2py setup\n",
    "import rpy2.rinterface_lib.callbacks\n",
    "import logging\n",
    "rpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)\n",
    "\n",
    "# anndata2ri.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "several-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "every-philip",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;1;31mSystem has not been booted with systemd as init system (PID 1). Can't operate.\u001b[0m\n",
      "\u001b[0;1;31mFailed to create bus connection: Host is down\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(tidyverse)\n",
    "library(reshape2)\n",
    "library(patchwork)\n",
    "\n",
    "remove_x_axis <- function(){\n",
    "  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank())  \n",
    "}\n",
    "\n",
    "remove_y_axis <- function(){\n",
    "  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank())  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "least-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/nfs/team205/ed6/data/PBMC_CZI_integration_filtered/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-producer",
   "metadata": {},
   "source": [
    "### Download data from studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "generous-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv('../../metadata/PBMC_study_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "least-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9415/4272191311.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metadata_df['file_path'].iloc[i] = file_path\n"
     ]
    }
   ],
   "source": [
    "metadata_df['file_path'] = np.nan\n",
    "for i in np.arange(metadata_df.shape[0]):\n",
    "    url = metadata_df['h5ad url'][i]\n",
    "    file_name = url.split(\"/\")[-1]\n",
    "    file_path = data_dir + file_name\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Downloading dataset {i+1}: {file_name}\\n\")\n",
    "        os.system(f'wget {url} -P {data_dir}')\n",
    "    if os.path.exists(file_path):\n",
    "        metadata_df['file_path'].iloc[i] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "surface-remains",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 1: 10_1038_s41591_021_01329_2.h5ad\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 2: ye_lupus_lupus.h5ad\n",
      "\n",
      "Downloading dataset 3: ye_lupus_normal.h5ad\n",
      "\n",
      "Downloading dataset 4: 10_1126_scitranslmed_abh2624.h5ad\n",
      "\n",
      "Downloading dataset 5: 10_1038_s41467_019_12464_3.h5ad\n",
      "\n",
      "Downloading dataset 6: 10_1101_2021_07_19_452956_blood.h5ad\n",
      "\n",
      "Downloading dataset 7: 10_1038_s41586_020_2157_4_blood.h5ad\n",
      "\n",
      "Downloading dataset 8: 10_1016_j_cell_2021_01_053_convalescence.h5ad\n",
      "\n",
      "Downloading dataset 9: 10_1016_j_cell_2021_01_053_nonconvalescence.h5ad\n",
      "\n",
      "Downloading dataset 10: 10_1038_s41591_020_0944_y.h5ad\n",
      "\n",
      "Downloading dataset 11: 10_1126_sciimmunol_abd1554.h5ad\n",
      "\n",
      "Downloading dataset 12: 10_1016_j_cell_2021_02_018_adaptive.h5ad\n",
      "\n",
      "Downloading dataset 13: 10_1016_j_cell_2021_02_018_innate.h5ad\n",
      "\n",
      "Downloading dataset 14: 10_1038_s41586_020_2922_4_10x.h5ad\n",
      "\n",
      "Downloading dataset 15: 10_1038_s41586_020_2922_4_smartseq2.h5ad\n",
      "\n",
      "Downloading dataset 16: 10_1016_j_cell_2022_01_012.h5ad\n",
      "\n",
      "Downloading dataset 17: 10_1016_j_cell_2020_08_001.h5ad\n",
      "\n",
      "Downloading dataset 18: 10_1126_science_abc6261.h5ad\n",
      "\n",
      "Downloading dataset 19: 10_1038_s41467_020_17834_w.h5ad\n",
      "\n",
      "Downloading dataset 20: powell_eqtl.h5ad\n",
      "\n",
      "Downloading dataset 21: 10_1038_s41586_021_04345_x.h5ad\n",
      "\n",
      "Downloading dataset 22: 10_1016_j_immuni_2021_03_005_49y2d.h5ad\n",
      "\n",
      "Downloading dataset 23: 10_1016_j_immuni_2021_03_005_49y3d.h5ad\n",
      "\n",
      "Downloading dataset 24: 10_1016_j_immuni_2021_03_005_66y2d.h5ad\n",
      "\n",
      "Downloading dataset 25: 10_1016_j_immuni_2021_03_005_66y4d.h5ad\n",
      "\n",
      "Downloading dataset 26: 10_1016_j_immuni_2021_03_005_74y3d.h5ad\n",
      "\n",
      "Downloading dataset 27: 10_1016_j_immuni_2021_03_005_74y4d.h5ad\n",
      "\n",
      "Downloading dataset 28: 10_1016_j_immuni_2021_03_005_74y7d.h5ad\n",
      "\n",
      "Downloading dataset 29: 10_1016_j_immuni_2021_03_005_66y3d.h5ad\n",
      "\n",
      "Downloading dataset 30: 10_1016_j_immuni_2021_03_005_74y8d.h5ad\n",
      "\n",
      "Downloading dataset 31: 10_1016_j_immuni_2021_03_005_49y1d.h5ad\n",
      "\n",
      "Downloading dataset 32: 10_1016_j_immuni_2021_03_005_82y1d.h5ad\n",
      "\n",
      "Downloading dataset 33: 10_1016_j_immuni_2021_03_005_66y7d.h5ad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metadata_df['file_path'] = np.nan\n",
    "for i in np.arange(metadata_df.shape[0]):\n",
    "    url = metadata_df['h5ad url'][i]\n",
    "    file_name = url.split(\"/\")[-1]\n",
    "    file_path = data_dir + file_name\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Downloading dataset {i+1}: {file_name}\\n\")\n",
    "        os.system(f'wget {url} -P {data_dir}')\n",
    "    if os.path.exists(file_path):\n",
    "        metadata_df['file_path'].iloc[i] = file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-partnership",
   "metadata": {},
   "source": [
    "### Collect sample info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "eleven-aircraft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 1\n",
      "Loading dataset 2\n",
      "Loading dataset 3\n",
      "Loading dataset 4\n",
      "Loading dataset 5\n",
      "Loading dataset 6\n",
      "Loading dataset 7\n",
      "Loading dataset 8\n",
      "Loading dataset 9\n",
      "Loading dataset 10\n",
      "Loading dataset 11\n",
      "Loading dataset 12\n",
      "Loading dataset 13\n",
      "Loading dataset 14\n",
      "Loading dataset 15\n",
      "Loading dataset 16\n",
      "Loading dataset 17\n",
      "Loading dataset 18\n",
      "Loading dataset 19\n",
      "Loading dataset 20\n",
      "Loading dataset 21\n",
      "Loading dataset 22\n",
      "Loading dataset 23\n",
      "Loading dataset 24\n",
      "Loading dataset 25\n",
      "Loading dataset 26\n",
      "Loading dataset 27\n",
      "Loading dataset 28\n",
      "Loading dataset 29\n",
      "Loading dataset 30\n",
      "Loading dataset 31\n",
      "Loading dataset 32\n",
      "Loading dataset 33\n"
     ]
    }
   ],
   "source": [
    "sample_obs_columns = ['sex', 'tissue', 'ethnicity', 'disease', 'assay', 'assay_ontology_term_id','sample_id', 'donor_id', 'dataset_id', 'development_stage']\n",
    "cell_obs_columns = ['cell_type']\n",
    "sample_obs_all = pd.DataFrame()\n",
    "for i in np.arange(metadata_df.shape[0]):\n",
    "    print(f\"Loading dataset {i+1}\")\n",
    "    adata = sc.read_h5ad(metadata_df['file_path'][i], backed='r')\n",
    "    sample_id_col = metadata_df['sample identifier column'].iloc[i]\n",
    "    if ' + ' in sample_id_col:\n",
    "        adata.obs['sample_id'] = adata.obs[sample_id_col.split(\" + \")].astype(\"str\").agg('-'.join, axis=1)\n",
    "    else:\n",
    "        adata.obs['sample_id'] = adata.obs[sample_id_col].values\n",
    "    adata.obs['donor_id'] = adata.obs[metadata_df['donor identifier column'].iloc[i]]\n",
    "    adata.obs['dataset_id'] = metadata_df['Dataset ID'].iloc[i]\n",
    "    sample_obs = adata.obs[sample_obs_columns].groupby('sample_id').first()\n",
    "    sample_obs['n_cells'] = adata.obs[sample_obs_columns].groupby('sample_id').size()\n",
    "    sample_obs.index = sample_obs.index.astype('str')\n",
    "    sample_obs_all = pd.concat([sample_obs_all, sample_obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "announced-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_obs_all.to_csv('../../metadata/PBMC_sample_metadata.csv')\n",
    "# metadata_df.to_csv('../../metadata/PBMC_study_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "after-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_obs_all = pd.read_csv('../../metadata/PBMC_sample_metadata.csv', index_col=0)\n",
    "metadata_df = pd.read_csv('../../metadata/PBMC_study_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-relevance",
   "metadata": {},
   "source": [
    "### Filter data for merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interim-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep 10X genomics data\n",
    "keep_sample_obs = sample_obs_all[sample_obs_all.assay.str.startswith('10x')]\n",
    "\n",
    "## Keep samples with at least 500 cells\n",
    "keep_sample_obs = keep_sample_obs[keep_sample_obs.n_cells > 500]\n",
    "\n",
    "## Split by disease\n",
    "normal_sample_obs = keep_sample_obs[keep_sample_obs.disease == 'normal']\n",
    "covid_sample_obs = keep_sample_obs[keep_sample_obs.disease == 'COVID-19']\n",
    "lupus_sample_obs = keep_sample_obs[keep_sample_obs.disease == 'systemic lupus erythematosus']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dirty-reason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1248,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_sample_obs.donor_id.unique().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normal_sample_obs.to_csv(data_dir + 'PBMC_sample_metadata.normal.csv')\n",
    "covid_sample_obs.to_csv(data_dir + 'PBMC_sample_metadata.COVID.csv')\n",
    "lupus_sample_obs.to_csv(data_dir + 'PBMC_sample_metadata.lupus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "featured-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_sample_obs = pd.read_csv(data_dir + 'PBMC_sample_metadata.normal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-examination",
   "metadata": {},
   "source": [
    "### Make merged objects\n",
    "\n",
    "Anndata objects are filtered (subsampling to 500 cells per sample) and split by condition running:\n",
    "```bash\n",
    "dataset_ids=$(cat /nfs/team205/ed6/data/PBMC_CZI_integration_filtered/PBMC_sample_metadata.normal.csv | tail -n +2 | cut -f 9 -d ','| sort | uniq)\n",
    "for d in $dataset_ids; do\n",
    "    python split_PBMC_dataset.py ${d} normal\n",
    "    done\n",
    "\n",
    "dataset_ids=$(cat /nfs/team205/ed6/data/PBMC_CZI_integration_filtered/PBMC_sample_metadata.COVID.csv | tail -n +2 | cut -f 9 -d ','| sort | uniq)\n",
    "for d in $dataset_ids; do\n",
    "    python split_PBMC_dataset.py ${d} COVID\n",
    "    done\n",
    "\n",
    "dataset_ids=$(cat /nfs/team205/ed6/data/PBMC_CZI_integration_filtered/PBMC_sample_metadata.lupus.csv | tail -n +2 | cut -f 9 -d ','| sort | uniq)\n",
    "for d in $dataset_ids; do\n",
    "    python split_PBMC_dataset.py ${d} lupus\n",
    "    done\n",
    "```\n",
    "\n",
    "Filtered AnnData objects are stored in `/nfs/team205/ed6/data/PBMC_CZI_integration_filtered/tmp/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "quick-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/nfs/team205/ed6/data/PBMC_CZI_integration_filtered/'\n",
    "tmp_dir = '/nfs/team205/ed6/data/PBMC_CZI_integration_filtered/tmp/'\n",
    "h5ad_files_normal = [x for x in os.listdir(tmp_dir) if x.endswith('.normal.subsample500cells.h5ad')]\n",
    "\n",
    "adata_ls = [sc.read_h5ad(tmp_dir + f) for f in h5ad_files_normal]\n",
    "\n",
    "## Make obs_names unique\n",
    "for a in adata_ls:\n",
    "    a.obs_names = a.obs['dataset_id'].astype('str') + '-' + a.obs_names.astype(\"str\")\n",
    "\n",
    "# check X stores raw counts \n",
    "def _check_counts_in_X(adata):\n",
    "    return(all(np.random.choice(adata.X.data, 100) % 1 == 0))\n",
    "\n",
    "if not all([_check_counts_in_X(a) for a in adata_ls]):\n",
    "    raise ValueError(\"Some matrix is not storing raw counts\")\n",
    "\n",
    "## Filter genes not expressed anywhere\n",
    "for a in adata_ls:\n",
    "    sc.pp.filter_genes(a, min_cells=1)\n",
    "\n",
    "## Concatenate\n",
    "adata_normal = anndata.concat(adata_ls)\n",
    "\n",
    "## Make var with gene names\n",
    "adata_normal.var['gene_id'] = adata_normal.var_names.values\n",
    "adata_normal.var['gene_name'] = [a for a in adata_ls if 'feature_name' in a.var.columns][0].var['feature_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "chemical-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save\n",
    "adata_normal.obs['donor_id'] = adata_normal.obs['donor_id'].astype('category')\n",
    "adata_normal.write_h5ad(data_dir + 'PBMC_merged.normal.subsample500cells.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-recovery",
   "metadata": {},
   "source": [
    "## Prep supplementary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "instrumental-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '/lustre/scratch117/cellgen/team205/ed6/PBMC_CZI_integration_filtered/'\n",
    "adata_full = sc.read_h5ad(outdir + 'PBMC_merged.normal.subsample500cells.clean_celltypes.h5ad', backed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "swedish-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_metadata = pd.read_csv('../../metadata/PBMC_study_metadata.csv', index_col=0)\n",
    "study_metadata = study_metadata[study_metadata['Dataset ID'].isin(sample_metadata.dataset_id)][['Dataset ID', 'DOI', 'assay', 'disease']].drop_duplicates()\n",
    "\n",
    "sample_metadata = pd.read_csv(data_dir + 'PBMC_sample_metadata.normal.csv', index_col=0)\n",
    "sample_metadata = sample_metadata.reset_index()\n",
    "sample_metadata = sample_metadata[sample_metadata.sample_id.isin(adata_full.obs['sample_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "actual-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_metadata.columns = ['dataset_id', 'DOI', 'assay', 'disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ancient-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix dataset naming\n",
    "study_metadata['dataset_id'] = [x[0] for x in study_metadata['dataset_id'].str.split(\"_innate\")]\n",
    "study_metadata['dataset_id'] = [x[0] for x in study_metadata['dataset_id'].str.split(\"_adaptive\")]\n",
    "study_metadata = study_metadata.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "minute-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells = pd.merge(adata_full.obs.reset_index(), study_metadata, on='dataset_id').groupby('DOI').size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "precious-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells.columns = ['DOI', 'n_cells']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "beginning-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = pd.merge(sample_metadata, study_metadata, on='dataset_id').groupby('DOI').size().reset_index()\n",
    "n_samples.columns = ['DOI', 'n_samples']\n",
    "study_table = study_metadata[['DOI', 'assay', 'disease']].drop_duplicates()\n",
    "study_table = pd.merge(pd.merge(study_table, n_samples), n_cells).sort_values('n_samples', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ranking-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_table.to_csv('/home/jovyan/mount/gdrive/diff2atlas/suppl_table_studies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "consistent-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(sample_metadata, study_metadata[['DOI', 'dataset_id']], on='dataset_id').to_csv('/home/jovyan/mount/gdrive/diff2atlas/suppl_table_samples.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (oor-benchmark)",
   "language": "python",
   "name": "oor-benchmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "438636853dc3f2e616d8497f3fdabe52cf4713e6895bec2ed2ae6e6315654a1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
